# Implement Command

Execute a planned implementation based on a structured YAML document generated by the `/p:task-plan` command.

# Usage

```bash
/p:implement [path_to_yaml]
```

- `path_to_yaml`: Optional path to the implementation plan YAML file (default: `requirements.yaml` in project root)

# Purpose

This command takes a completed implementation plan (from `/p:task-plan`) and executes it step-by-step:
- Reads the YAML document with requirements and p:implementation_plan
- Validates the plan is complete and ready for implementation
- Executes tasks in dependency order
- Tests each task after completion
- Reports progress and handles errors

# Prerequisites

Before running this command:
1. A complete implementation plan must exist (generated by `/p:task-plan` command)
2. The YAML file must have `complete: true`
3. The `p:implementation_plan` section must be present with tasks
4. All code references in the plan should be valid

# Code References Usage

**CRITICAL**: The implementation plan contains `code_references` for each task. These references are NOT suggestions - they are MANDATORY patterns to follow.

## Why code references exist

The `/p:task-plan` command searched the codebase and identified similar implementations that:
- Solve comparable problems
- Use the same architectural patterns
- Follow the project's conventions
- Demonstrate the correct approach

## How to use code references

For EVERY task with `code_references`:

1. **Read ALL referenced code** before writing anything
2. **Understand the pattern**: Why was this code written this way? What pattern does it follow?
3. **Apply the SAME approach**: Don't invent a new solution if a pattern exists
4. **Match the style**: Function signatures, error handling, memory management, naming conventions
5. **Adapt, don't copy**: Use the same pattern but adapted to your specific requirements

## What NOT to do

- ❌ Ignore code references and write from scratch
- ❌ Skim references without understanding the pattern
- ❌ Use a different approach "because it's better" - follow existing patterns for consistency
- ❌ Copy-paste without understanding

## Example

If `code_references` points to `websocket_send_frame()` for implementing `websocket_send_ping()`:
- ✅ Use the same parameter validation approach
- ✅ Use the same error handling pattern
- ✅ Use the same return value convention
- ✅ Use the same memory management style
- ✅ Use the same logging approach

# Workflow

## 1. Initialization

0. Don't forget read that fuckin' CLAUDE.md and relevant language-specific instruction files!
1. **Load implementation plan using `/p:implementation-plan` skill**
   - This loads only the essential data (complete flag, success_criteria, p:implementation_plan)
   - Token-efficient: excludes original_request, requirements, constraints
   - If path needed: use `--plan <path>` parameter (default: requirements.yaml in project root)
2. Validate structure:
   - Check `complete: true`
   - Verify `p:implementation_plan` section exists
   - Verify `tasks` array is not empty
3. **Check task status in requirements.yaml**:
   - Each task has a `status` field: `pending`, `in_progress`, `completed`, or `cancel`
   - Skip tasks that are already `completed`
   - Continue from tasks that are `pending` or `in_progress`
   - This allows resuming interrupted implementations
4. **Read ALL source code files in `reference_files`** - these contain the **code patterns** you MUST follow (any language)
5. **Read ALL documentation in `api_references`** - these explain the APIs and architecture
6. Create a dependency graph from task dependencies
7. Determine task execution order (topological sort), excluding already completed tasks

## 2. Task Execution Loop

For each task in dependency order:

### a. Pre-task validation
- Verify all dependent tasks are completed successfully
- Check if file exists (for modify/delete operations)
- **MANDATORY**: Read ALL code in `code_references` - understand the pattern before coding
- Review the `note` field for each reference to understand WHY it's relevant
- **Mark task as in_progress**: `python3 ~/.claude/skills/p:requirements/update_tasks.py in_progress <task_id>`

### b. Task execution

Based on task `type`:

**create:**
- Read ALL `code_references` first - understand the existing patterns
- If `function_name` specified: Create the function following the EXACT pattern from references:
  - Same parameter validation approach
  - Same error handling pattern (return values, error codes)
  - Same memory management style (allocation/deallocation)
  - Same logging approach
  - Same documentation style (Doxygen comments)
- If `function_name` is null: Modify file content (add constants, types, etc.) matching reference style
- Apply project code style guidelines (CLAUDE.md)
- **DO NOT invent new patterns** - consistency is more important than "better" solutions

**modify:**
- Read the existing file
- Read ALL `code_references` to understand the pattern being applied
- Locate the function (if `function_name` specified)
- Apply modifications according to `implementation_details`
- Preserve existing code style
- **Match the pattern from references** - same approach, same style, adapted to this context

**delete:**
- Remove the specified function or code section
- Clean up any orphaned dependencies
- Check `code_references` for proper cleanup patterns

**test:**
- Read ALL `code_references` for test patterns
- Create or modify test file following the EXACT test structure from references:
  - Same test framework usage (CTEST/CTEST2 macros)
  - Same setup/teardown patterns
  - Same assertion style
  - Same test naming conventions
- Implement test cases as described in `implementation_details`
- **DO NOT invent new test patterns** - follow existing test structure

### c. Post-task verification

After each task:
1. **Language-specific code quality checks**:
   - **For C/C++ files**: Run `clang-tidy -p build <file_path>` and fix all warnings
   - **For Lua files**: Check syntax with `luac -p <file_path>`
   - **For Python files**: Run relevant linters if configured (e.g., pylint, flake8)
2. **Build**: Run `cmake --build build` to verify compilation
3. **Run tests**: If the task type is `test` or creates test files:
   - Run the specific test: `build/src/tests/[test-application] [suite:test]`
   - Verify test passes
4. **Validate**: Check that `test_requirements` are met
5. **Mark task as completed** (ONLY if all above checks pass): `python3 ~/.claude/skills/p:requirements/update_tasks.py completed <task_id>`
   - DO NOT mark completed if any check fails (build errors, test failures, linting warnings)
   - DO NOT mark completed if implementation is incomplete
   - Only mark completed when task is fully done and verified

### d. Error handling

If any verification step fails:
- Report the error with context (task_id, file, function)
- Show compiler/test output
- **DO NOT mark task as completed** - leave it as `in_progress`
- Ask user whether to:
  1. Fix the error automatically (keep task `in_progress`)
  2. Skip this task and continue (keep task `in_progress`)
  3. Abort implementation (keep task `in_progress`)
- DO NOT proceed to next task until current task is verified
- Only mark `completed` when ALL verification steps pass

### e. Progress tracking

- Use TodoWrite to track task progress in the current session
- **Automatically update task status in requirements.yaml**:
  - Mark `in_progress` when starting each task (pre-task validation step)
  - Mark `completed` when task is fully verified (post-task verification step)
  - This ensures requirements.yaml always reflects current implementation state
- Keep user informed of progress
- If using `/p:requirements` skill, the updated task status will be visible in the task list

## 3. Post-implementation

After all tasks are completed:

1. Run full test suite: `ctest --test-dir build`
2. Verify all `success_criteria` from the YAML are met
3. Generate implementation summary:
   - List of modified files
   - List of new files
   - List of completed tasks
   - Test results
   - Any warnings or issues encountered

4. Update the YAML file:
   - Add `implementation_complete: true`
   - Add `implementation_date: YYYY-MM-DD`
   - Add any notes about implementation deviations

## 4. Quality Checks

Before marking implementation complete:

- All tasks executed successfully
- All tests pass
- No language-specific linting warnings (clang-tidy for C/C++, luac for Lua, etc.)
- Build succeeds without errors
- Success criteria are met
- Code follows project style guidelines (CLAUDE.md and language-specific instructions)

# Error Recovery

If implementation is interrupted:
- The TodoWrite task list shows progress in the current session
- **Task status is automatically saved in requirements.yaml**:
  - `completed` tasks are automatically marked and will be skipped on resume
  - `in_progress` tasks will be re-executed
  - `pending` tasks haven't been started yet
- Re-run `/p:implement` to continue from where it stopped
- The command automatically skips completed tasks based on status in requirements.yaml
- Use `/p:requirements` to view current task status before resuming

# Important Notes

- **Autonomous execution**: This command should work without user intervention for well-defined tasks
- **Automatic task status tracking**: Each task is automatically marked as:
  - `in_progress` when starting (before implementation)
  - `completed` when fully verified (after all checks pass)
  - Status updates are written to requirements.yaml using the p:requirements skill
  - This provides real-time visibility into implementation progress
- **Test-driven**: Each task must be tested before proceeding
- **Style compliance**: Follow CLAUDE.md and language-specific instructions (use specific skills)
- **Language-aware**: Apply appropriate code quality tools and conventions based on file language
- **Incremental**: Build and test after each task, not at the end
- **Documentation**: Reference implementation details from the YAML, don't guess
- **No shortcuts**: Don't skip verification steps to save time
- **Ask when unclear**: If implementation_details are ambiguous, ask before coding
- **Status discipline**: Only mark tasks `completed` when ALL verification criteria are met (tests pass, build succeeds, linting passes)

# Example Execution Flow

```
[Load implementation plan via /p:implementation-plan]
✓ Plan is complete
✓ Found 4 tasks in p:implementation_plan
✓ Dependency order: task-001 → task-002 → task-003 → task-004

[Task 1/4: task-001]
Description: Add ping/pong frame type constants to WebSocket header
File: /path/to/file/websocket-server.h
✓ Marked as in_progress
✓ Read reference code
✓ Modified file
✓ clang-tidy passed
✓ Build successful
✓ Marked as completed

[Task 2/4: task-002]
Description: Implement websocket_send_ping function
File: /path/to/file/websocket-server.c
Function: websocket_send_ping
✓ Marked as in_progress
✓ Read reference code: websocket_send_frame
✓ Created function
✓ clang-tidy passed
✓ Build successful
✓ Marked as completed

[Task 3/4: task-003]
Description: Handle incoming ping frames and auto-respond with pong
File: /path/to/file/websocket-server.c
Function: websocket_handle_frame
✓ Marked as in_progress
✓ Read existing function
✓ Modified function
✓ clang-tidy passed
✓ Build successful
✓ Marked as completed

[Task 4/4: task-004]
Description: Create integration test for ping/pong functionality
File: /path/to/file/test-websocket-ping-pong.c
✓ Marked as in_progress
✓ Created test file
✓ clang-tidy passed
✓ Build successful
✓ Tests passed (3/3)
✓ Marked as completed

[Final verification]
✓ Full test suite passed
✓ All success criteria met

Implementation complete!
Modified files: 2
New files: 1
Total tasks: 4
```

# Command Parameters

The command accepts optional parameters:

- `--plan <path>`: Path to YAML plan file (default: requirements.yaml)
- `--dry-run`: Show execution plan without making changes
- `--continue`: Continue from last completed task (resume interrupted implementation)
- `--task <task_id>`: Execute only specific task (for debugging)

Example:
```
/p:implement --plan ./docs/feature-x-plan.yaml
/p:implement --continue
/p:implement --task task-003
```

# Checking Task Status

Before or during implementation, you can check task status:
```
/p:requirements                    # Show all tasks with current status
python3 ~/.claude/skills/p:requirements/show_tasks.py requirements.yaml
python3 ~/.claude/skills/p:requirements/show_task_details.py task-001 task-002
```

This helps you understand:
- Which tasks are already completed
- Which tasks are in progress
- Which tasks are still pending
- Overall implementation progress
